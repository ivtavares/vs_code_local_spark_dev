{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Inicialize a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"ExemploParticionamento\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie um DataFrame de exemplo\n",
    "data = [(\"2023-01-01\", \"ProdutoA\", \"RegiãoA\", 100),\n",
    "        (\"2023-01-02\", \"ProdutoB\", \"RegiãoB\", 150),\n",
    "        (\"2023-01-03\", \"ProdutoA\", \"RegiãoA\", 200),\n",
    "        (\"2023-01-04\", \"ProdutoC\", \"RegiãoB\", 120),\n",
    "        (\"2023-01-05\", \"ProdutoB\", \"RegiãoC\", 180),\n",
    "        (\"2023-01-06\", \"ProdutoA\", \"RegiãoB\", 90)]\n",
    "\n",
    "columns = [\"data\", \"produto\", \"regiao\", \"valor\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-----+\n",
      "|      data| produto| regiao|valor|\n",
      "+----------+--------+-------+-----+\n",
      "|2023-01-01|ProdutoA|RegiãoA|  100|\n",
      "|2023-01-02|ProdutoB|RegiãoB|  150|\n",
      "|2023-01-03|ProdutoA|RegiãoA|  200|\n",
      "|2023-01-04|ProdutoC|RegiãoB|  120|\n",
      "|2023-01-05|ProdutoB|RegiãoC|  180|\n",
      "|2023-01-06|ProdutoA|RegiãoB|   90|\n",
      "+----------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exiba o DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Particionado:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-----+\n",
      "|      data| produto| regiao|valor|\n",
      "+----------+--------+-------+-----+\n",
      "|2023-01-01|ProdutoA|RegiãoA|  100|\n",
      "|2023-01-03|ProdutoA|RegiãoA|  200|\n",
      "|2023-01-02|ProdutoB|RegiãoB|  150|\n",
      "|2023-01-04|ProdutoC|RegiãoB|  120|\n",
      "|2023-01-06|ProdutoA|RegiãoB|   90|\n",
      "|2023-01-05|ProdutoB|RegiãoC|  180|\n",
      "+----------+--------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Particione o DataFrame por três colunas: data, produto e região\n",
    "df_particionado = df.repartition(\"data\", \"produto\", \"regiao\")\n",
    "\n",
    "# Exiba o DataFrame particionado\n",
    "print(\"\\nDataFrame Particionado:\")\n",
    "df_particionado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de Partições: 1\n"
     ]
    }
   ],
   "source": [
    "# Mostre o número de partições\n",
    "print(f\"\\nNúmero de Partições: {df_particionado.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerre a sessão do Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/05 05:08:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|   name|partition|\n",
      "+---+-------+---------+\n",
      "|  1|  Alice|        A|\n",
      "|  3|Charlie|        A|\n",
      "+---+-------+---------+\n",
      "\n",
      "+---+-----+---------+\n",
      "| id| name|partition|\n",
      "+---+-----+---------+\n",
      "|  2|  Bob|        B|\n",
      "|  4|David|        B|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Suponha que df seja o seu DataFrame principal\n",
    "data = [(1, 'Alice', 'A'), (2, 'Bob', 'B'), (3, 'Charlie', 'A'), (4, 'David', 'B')]\n",
    "columns = ['id', 'name', 'partition']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Obtém as partições distintas presentes no DataFrame principal\n",
    "partitions = df.select('partition').distinct().collect()\n",
    "\n",
    "# Cria DataFrames distintos para cada partição\n",
    "dataframes = {}\n",
    "for partition in partitions:\n",
    "    partition_value = partition['partition']\n",
    "    partition_df = df.filter(df['partition'] == partition_value)\n",
    "    dataframes[partition_value] = partition_df\n",
    "\n",
    "# Exemplo de como acessar um DataFrame específico\n",
    "if 'A' in dataframes:\n",
    "    dataframe_A = dataframes['A']\n",
    "    dataframe_A.show()\n",
    "\n",
    "if 'B' in dataframes:\n",
    "    dataframe_B = dataframes['B']\n",
    "    dataframe_B.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
